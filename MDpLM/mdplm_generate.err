nohup: ignoring input
/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
scaffolding (uppercase):   0%|          | 0/5 [00:00<?, ?it/s]scaffolding (uppercase):   0%|          | 0/5 [00:12<?, ?it/s]
Error executing job with overrides: []
Traceback (most recent call last):
  File "/workspace/sg666/MDpLM/mdlm_motif_benchmarking.py", line 72, in mdlm_motif_benchmark
    cos_sim = calculate_cosine_sim(original_sequence, generated_sequence, tokenizer, esm_model, device)
  File "/workspace/sg666/MDpLM/mlm_generate_utils.py", line 97, in calculate_cosine_sim
    og_embeddings = get_latents(esm_model, tokenizer, original_sequence.upper()).to(device)
  File "/workspace/sg666/MDpLM/esm_utils.py", line 14, in get_latents
    logits = model(**inputs).logits
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/transformers/models/esm/modeling_esm.py", line 907, in forward
    embedding_output = self.embeddings(
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/transformers/models/esm/modeling_esm.py", line 203, in forward
    inputs_embeds = self.word_embeddings(input_ids)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py", line 163, in forward
    return F.embedding(
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py", line 2237, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
