Seed set to 42
/home/sg666/miniconda3/envs/shrey_mdlm/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[rank: 0] Seed set to 42
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/3
[rank: 2] Seed set to 42
/home/sg666/miniconda3/envs/shrey_mdlm/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[rank: 1] Seed set to 42
/home/sg666/miniconda3/envs/shrey_mdlm/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[rank: 2] Seed set to 42
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/3
[rank: 1] Seed set to 42
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/3
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 3 processes
----------------------------------------------------------------------------------------------------

wandb: Currently logged in as: shrey-goel (programmablebio). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.19.8 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.5
wandb: Run data is saved locally in ./wandb/run-20250315_140130-esm-300m_omg1m-2800_epochs10_lr3e-5_wd0.01_bsz16_gradclip0.75_betas0.9-0.999_42
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run esm-300m_omg1m-2800_epochs10_lr3e-5_wd0.01_bsz16_gradclip0.75_betas0.9-0.999
wandb: ‚≠êÔ∏è View project at https://wandb.ai/programmablebio/MeMDLM_omg_pretrain
wandb: üöÄ View run at https://wandb.ai/programmablebio/MeMDLM_omg_pretrain/runs/esm-300m_omg1m-2800_epochs10_lr3e-5_wd0.01_bsz16_gradclip0.75_betas0.9-0.999_42
/home/sg666/miniconda3/envs/shrey_mdlm/lib/python3.9/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /raid/sg666/MeMDLM/MeMDLM/checkpoints/checkpoints exists and is not empty.
Restoring states from the checkpoint path at /raid/sg666/MeMDLM/MeMDLM/checkpoints/checkpoints/3-125000.ckpt
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1,2,3]

  | Name           | Type             | Params
----------------------------------------------------
0 | noise          | LogLinearNoise   | 0     
1 | backbone       | WrapESM          | 292 M 
2 | train_metrics  | MetricCollection | 0     
3 | valid_metrics  | MetricCollection | 0     
4 | test_metrics   | MetricCollection | 0     
5 | gen_ppl_metric | Perplexity       | 0     
----------------------------------------------------
292 M     Trainable params
0         Non-trainable params
292 M     Total params
1,168.142 Total estimated model params size (MB)
Restored all states from the checkpoint at /raid/sg666/MeMDLM/MeMDLM/checkpoints/checkpoints/3-125000.ckpt
/home/sg666/miniconda3/envs/shrey_mdlm/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py:161: You're resuming from a checkpoint that ended before the epoch ended and your dataloader is not resumable. This can cause unreliable results if further training is done. Consider using an end-of-epoch checkpoint or make your dataloader resumable by implementing the `state_dict` / `load_state_dict` interface.
