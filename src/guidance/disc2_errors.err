nohup: ignoring input
Some weights of EsmModel were not initialized from the model checkpoint at /workspace/sg666/MDpLM/checkpoints/150M/membrane_automodel/epochs60_lr3e-4_200k-seqs_bsz16_all-params_no-compile_gradclip1_beta-one0.9_beta-two0.999_bf16 and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of EsmModel were not initialized from the model checkpoint at /workspace/sg666/MDpLM/checkpoints/150M/membrane_automodel/epochs60_lr3e-4_200k-seqs_bsz16_all-params_no-compile_gradclip1_beta-one0.9_beta-two0.999_bf16 and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of EsmModel were not initialized from the model checkpoint at /workspace/sg666/MDpLM/checkpoints/150M/membrane_automodel/epochs60_lr3e-4_200k-seqs_bsz16_all-params_no-compile_gradclip1_beta-one0.9_beta-two0.999_bf16 and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
wandb: Currently logged in as: shrey-goel (programmablebio). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.5
wandb: Run data is saved locally in /workspace/sg666/MeMDLM/MeMDLM/src/guidance/wandb/run-20250117_043635-mtohxd49
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run steps50k_lr3e-5_bsz16_heads2_drpt0.5_layers4_mask0.50
wandb: ‚≠êÔ∏è View project at https://wandb.ai/programmablebio/MeMDLM_Guidance
wandb: üöÄ View run at https://wandb.ai/programmablebio/MeMDLM_Guidance/runs/mtohxd49
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/workspace/sg666/MeMDLM/MeMDLM/src/guidance/main.py:187: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(ckpt_path, map_location='cuda' if torch.cuda.is_available() else 'cpu')
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 1 processes
----------------------------------------------------------------------------------------------------

/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loggers/wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
Restoring states from the checkpoint path at /workspace/sg666/MeMDLM/MeMDLM/checkpoints/classifier/steps50k_lr3e-5_bsz16_heads2_drpt0.5_layers4_mask0.50/best_model.ckpt
/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/cloud_io.py:57: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
Loaded model weights from the checkpoint at /workspace/sg666/MeMDLM/MeMDLM/checkpoints/classifier/steps50k_lr3e-5_bsz16_heads2_drpt0.5_layers4_mask0.50/best_model.ckpt
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:409: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at ../aten/src/ATen/NestedTensorImpl.cpp:178.)
  output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not(), mask_check=False)
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:               epoch ‚ñÅ
wandb:          test/AUROC ‚ñÅ
wandb:       test/accuracy ‚ñÅ
wandb:           test/loss ‚ñÅ
wandb: trainer/global_step ‚ñÅ
wandb: 
wandb: Run summary:
wandb:               epoch 0
wandb:          test/AUROC 0.92392
wandb:       test/accuracy 0.85336
wandb:           test/loss 0.34105
wandb: trainer/global_step 0
wandb: 
wandb: Synced steps50k_lr3e-5_bsz16_heads2_drpt0.5_layers4_mask0.50: https://wandb.ai/programmablebio/MeMDLM_Guidance/runs/mtohxd49
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250117_043635-mtohxd49/logs
