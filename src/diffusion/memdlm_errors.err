Seed set to 42
/home/sg666/miniconda3/envs/shrey_mdlm/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
You are using a CUDA device ('NVIDIA RTX 6000 Ada Generation') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[rank: 0] Seed set to 42
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/7
[rank: 3] Seed set to 42
/home/sg666/miniconda3/envs/shrey_mdlm/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[rank: 2] Seed set to 42
/home/sg666/miniconda3/envs/shrey_mdlm/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[rank: 5] Seed set to 42
/home/sg666/miniconda3/envs/shrey_mdlm/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[rank: 4] Seed set to 42
[rank: 1] Seed set to 42
/home/sg666/miniconda3/envs/shrey_mdlm/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/sg666/miniconda3/envs/shrey_mdlm/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[rank: 6] Seed set to 42
/home/sg666/miniconda3/envs/shrey_mdlm/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[rank: 1] Seed set to 42
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/7
[rank: 3] Seed set to 42
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/7
[rank: 6] Seed set to 42
Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/7
[rank: 2] Seed set to 42
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/7
[rank: 5] Seed set to 42
Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/7
[rank: 4] Seed set to 42
Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/7
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 7 processes
----------------------------------------------------------------------------------------------------

wandb: Currently logged in as: shrey-goel (programmablebio). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.8 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.5
wandb: Run data is saved locally in ./wandb/run-20250305_125011-test_42
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run test
wandb: ‚≠êÔ∏è View project at https://wandb.ai/programmablebio/MeMDLM_omg_pretrain
wandb: üöÄ View run at https://wandb.ai/programmablebio/MeMDLM_omg_pretrain/runs/test_42
/home/sg666/miniconda3/envs/shrey_mdlm/lib/python3.9/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/sg666/MeMDLM/MeMDLM/checkpoints/checkpoints exists and is not empty.
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]

  | Name           | Type             | Params
----------------------------------------------------
0 | backbone       | WrapESM          | 149 M 
1 | train_metrics  | MetricCollection | 0     
2 | valid_metrics  | MetricCollection | 0     
3 | test_metrics   | MetricCollection | 0     
4 | gen_ppl_metric | Perplexity       | 0     
5 | noise          | LogLinearNoise   | 0     
----------------------------------------------------
149 M     Trainable params
0         Non-trainable params
149 M     Total params
597.803   Total estimated model params size (MB)
/home/sg666/miniconda3/envs/shrey_mdlm/lib/python3.9/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: Encountered `nan` values in tensor. Will be removed.
  warnings.warn(*args, **kwargs)  # noqa: B028
Epoch 0, global step 15: 'val/nll' reached 0.00000 (best 0.00000), saving model to '/home/sg666/MeMDLM/MeMDLM/checkpoints/checkpoints/best-v2.ckpt' as top 1
Error executing job with overrides: []
Traceback (most recent call last):
  File "/home/sg666/MeMDLM/MeMDLM/src/diffusion/main.py", line 216, in main
    _train(config, logger, tokenizer, data_module)
  File "/home/sg666/MeMDLM/MeMDLM/src/diffusion/main.py", line 150, in _train
    trainer.fit(model, datamodule=data_module, ckpt_path=ckpt_path)
  File "/home/sg666/miniconda3/envs/shrey_mdlm/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 544, in fit
    call._call_and_handle_interrupt(
  File "/home/sg666/miniconda3/envs/shrey_mdlm/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 43, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/sg666/miniconda3/envs/shrey_mdlm/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/home/sg666/miniconda3/envs/shrey_mdlm/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 580, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/sg666/miniconda3/envs/shrey_mdlm/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 987, in _run
    results = self._run_stage()
  File "/home/sg666/miniconda3/envs/shrey_mdlm/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1033, in _run_stage
    self.fit_loop.run()
  File "/home/sg666/miniconda3/envs/shrey_mdlm/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 206, in run
    self.on_advance_end()
  File "/home/sg666/miniconda3/envs/shrey_mdlm/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 378, in on_advance_end
    call._call_callback_hooks(trainer, "on_train_epoch_end", monitoring_callbacks=True)
  File "/home/sg666/miniconda3/envs/shrey_mdlm/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 208, in _call_callback_hooks
    fn(trainer, trainer.lightning_module, *args, **kwargs)
  File "/home/sg666/miniconda3/envs/shrey_mdlm/lib/python3.9/site-packages/lightning/pytorch/callbacks/model_checkpoint.py", line 324, in on_train_epoch_end
    self._save_topk_checkpoint(trainer, monitor_candidates)
  File "/home/sg666/miniconda3/envs/shrey_mdlm/lib/python3.9/site-packages/lightning/pytorch/callbacks/model_checkpoint.py", line 384, in _save_topk_checkpoint
    self._save_monitor_checkpoint(trainer, monitor_candidates)
  File "/home/sg666/miniconda3/envs/shrey_mdlm/lib/python3.9/site-packages/lightning/pytorch/callbacks/model_checkpoint.py", line 704, in _save_monitor_checkpoint
    self._update_best_and_save(current, trainer, monitor_candidates)
  File "/home/sg666/miniconda3/envs/shrey_mdlm/lib/python3.9/site-packages/lightning/pytorch/callbacks/model_checkpoint.py", line 756, in _update_best_and_save
    self._save_checkpoint(trainer, filepath)
  File "/home/sg666/miniconda3/envs/shrey_mdlm/lib/python3.9/site-packages/lightning/pytorch/callbacks/model_checkpoint.py", line 389, in _save_checkpoint
    trainer.save_checkpoint(filepath, self.save_weights_only)
  File "/home/sg666/miniconda3/envs/shrey_mdlm/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1380, in save_checkpoint
    checkpoint = self._checkpoint_connector.dump_checkpoint(weights_only)
  File "/home/sg666/miniconda3/envs/shrey_mdlm/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/checkpoint_connector.py", line 493, in dump_checkpoint
    call._call_lightning_module_hook(trainer, "on_save_checkpoint", checkpoint)
  File "/home/sg666/miniconda3/envs/shrey_mdlm/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 157, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/sg666/MeMDLM/MeMDLM/src/diffusion/diffusion.py", line 256, in on_save_checkpoint
    self.backbone.save_model(self.config.checkpointing.pretrained_esm_mdlm_automodel_path)
  File "/home/sg666/MeMDLM/MeMDLM/src/diffusion/diffusion.py", line 130, in save_model
    self.model.save_pretrained(save_dir)
  File "/home/sg666/miniconda3/envs/shrey_mdlm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 2448, in save_pretrained
    safe_save_file(shard, os.path.join(save_directory, shard_file), metadata={"format": "pt"})
  File "/home/sg666/miniconda3/envs/shrey_mdlm/lib/python3.9/site-packages/safetensors/torch.py", line 286, in save_file
    serialize_file(_flatten(tensors), filename, metadata=metadata)
  File "/home/sg666/miniconda3/envs/shrey_mdlm/lib/python3.9/site-packages/safetensors/torch.py", line 496, in _flatten
    return {
  File "/home/sg666/miniconda3/envs/shrey_mdlm/lib/python3.9/site-packages/safetensors/torch.py", line 500, in <dictcomp>
    "data": _tobytes(v, k),
  File "/home/sg666/miniconda3/envs/shrey_mdlm/lib/python3.9/site-packages/safetensors/torch.py", line 460, in _tobytes
    return data.tobytes()
  File "/home/sg666/miniconda3/envs/shrey_mdlm/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 2152091) is killed by signal: Killed. 

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
[rank: 6] Child process with PID 2148472 terminated with code 1. Forcefully terminating all other processes to avoid zombies üßü
wandb: ERROR Failed to sample metric: process no longer exists (pid=2148122)
Exception in thread MsgRouterThr:
Traceback (most recent call last):
  File "/home/sg666/miniconda3/envs/shrey_mdlm/lib/python3.9/threading.py", line 980, in _bootstrap_inner
    self.run()
  File "/home/sg666/miniconda3/envs/shrey_mdlm/lib/python3.9/threading.py", line 917, in run
    self._target(*self._args, **self._kwargs)
  File "/home/sg666/miniconda3/envs/shrey_mdlm/lib/python3.9/site-packages/wandb/sdk/interface/router.py", line 70, in message_loop
    msg = self._read_message()
  File "/home/sg666/miniconda3/envs/shrey_mdlm/lib/python3.9/site-packages/wandb/sdk/interface/router_queue.py", line 36, in _read_message
    msg = self._response_queue.get(timeout=1)
  File "/home/sg666/miniconda3/envs/shrey_mdlm/lib/python3.9/multiprocessing/queues.py", line 117, in get
    res = self._recv_bytes()
  File "/home/sg666/miniconda3/envs/shrey_mdlm/lib/python3.9/multiprocessing/connection.py", line 212, in recv_bytes
    self._check_closed()
  File "/home/sg666/miniconda3/envs/shrey_mdlm/lib/python3.9/multiprocessing/connection.py", line 136, in _check_closed
